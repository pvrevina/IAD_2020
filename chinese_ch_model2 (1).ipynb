{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chinese_ch_model2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXbPwvBF1GOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir ~/.kaggle2 && echo '{\"username\":\"polinarevina\",\"key\":\"5bb914194d8b5103c7fab5f72b7c1806\"}' >~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZe1MtVe1ZJO",
        "colab_type": "code",
        "outputId": "05ab8af5-b911-4a1b-9295-420bc3c7c6f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "!kaggle competitions download -c chinese-char-recognition-smmo19 "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading train-3.npy.zip to /content\n",
            " 99% 183M/185M [00:06<00:00, 23.3MB/s]\n",
            "100% 185M/185M [00:06<00:00, 28.7MB/s]\n",
            "Downloading train-1.npy.zip to /content\n",
            " 93% 105M/113M [00:01<00:00, 70.2MB/s] \n",
            "100% 113M/113M [00:01<00:00, 86.4MB/s]\n",
            "Downloading train-4.npy.zip to /content\n",
            " 94% 184M/195M [00:02<00:00, 80.3MB/s]\n",
            "100% 195M/195M [00:02<00:00, 96.1MB/s]\n",
            "Downloading test.npy.zip to /content\n",
            " 95% 191M/202M [00:02<00:00, 85.2MB/s]\n",
            "100% 202M/202M [00:02<00:00, 91.1MB/s]\n",
            "Downloading random_labels.csv to /content\n",
            "  0% 0.00/965k [00:00<?, ?B/s]\n",
            "100% 965k/965k [00:00<00:00, 123MB/s]\n",
            "Downloading train-2.npy.zip to /content\n",
            " 95% 157M/165M [00:03<00:00, 55.8MB/s]\n",
            "100% 165M/165M [00:03<00:00, 49.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8zjPW2H1a3Y",
        "colab_type": "code",
        "outputId": "9f49efc9-0172-4c08-a2f4-ae25fa96b3d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!unzip train-4.npy.zip \n",
        "!unzip train-3.npy.zip\n",
        "!unzip test.npy.zip\n",
        "!unzip train-2.npy.zip\n",
        "!unzip train-1.npy.zip\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train-4.npy.zip\n",
            "  inflating: train-4.npy             \n",
            "Archive:  train-3.npy.zip\n",
            "  inflating: train-3.npy             \n",
            "Archive:  test.npy.zip\n",
            "  inflating: test.npy                \n",
            "Archive:  train-2.npy.zip\n",
            "  inflating: train-2.npy             \n",
            "Archive:  train-1.npy.zip\n",
            "  inflating: train-1.npy             \n",
            "random_labels.csv  test.npy.zip     train-2.npy      train-3.npy.zip\n",
            "sample_data\t   train-1.npy\t    train-2.npy.zip  train-4.npy\n",
            "test.npy\t   train-1.npy.zip  train-3.npy      train-4.npy.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUNretL04cd4",
        "colab_type": "code",
        "outputId": "e64265d8-de18-4065-f5bd-86f556797fdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random_labels.csv  test.npy.zip     train-2.npy.zip  train-4.npy.zip\n",
            "sample_data\t   train-1.npy.zip  train-3.npy.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovbgZVpz1cnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98w3X69q1g71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train = np.load(\"train-1.npy\", allow_pickle = True)\n",
        "for i in range(2, 5): \n",
        "  data_train_n = np.load(f\"train-{i}.npy\", allow_pickle = True)\n",
        "  data_train = np.concatenate([data_train, data_train_n])\n",
        "data_test = np.load(\"test.npy\", allow_pickle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x6nrTE_1hBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128 \n",
        "val_size = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZDrv-Lj1n_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " char_to_id = {char: id for id, char in enumerate (np.unique([char for _, char in data_train]))}\n",
        " id_to_char = {id: char for id, char in enumerate (np.unique([char for _, char in data_train]))}\n",
        " def train_gen():\n",
        "    for img, label in data_train[int(len(data_train) * val_size):]:\n",
        "        img = img[:, :, np.newaxis] # [batch, w, h, channels]\n",
        "        yield img, char_to_id[label]   \n",
        "def val_gen():\n",
        "    for img, label in data_train[:int(len(data_train) * val_size)]:\n",
        "        img = img[:, :, np.newaxis] # [batch, w, h, channels]\n",
        "        yield img, char_to_id[label]\n",
        "def test_gen():\n",
        "    for img in data_test:\n",
        "        img = img[:, :, np.newaxis] # [batch, w, h, channels]\n",
        "        yield img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0QMGo4t1yJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_train = tf.data.Dataset.from_generator(train_gen,\n",
        "                                               output_types = (tf.uint8, tf.int32),\n",
        "                                               output_shapes = ((None, None, 1),()),\n",
        "                                               )\n",
        "dataset_val = tf.data.Dataset.from_generator(val_gen,\n",
        "                                               output_types = (tf.uint8, tf.int32),\n",
        "                                               output_shapes = ((None, None, 1),()),\n",
        "                                               )\n",
        "dataset_test = tf.data.Dataset.from_generator(test_gen,\n",
        "                                               output_types = tf.uint8,\n",
        "                                               output_shapes = ((None, None, 1)),\n",
        "                                               )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3XVHiQK32O1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def augmentation(img): \n",
        "  tf.image.random_brightness(img, 0.2)\n",
        "  return img \n",
        "\n",
        "def preprocess_train(img, label): \n",
        "  img = tf.image.resize(img, (64, 64))\n",
        "  img = tf.cast(img, tf.float32) / 127.5 - 1\n",
        "  if np.random.rand() < 0.2: \n",
        "    img = augmentation(img)\n",
        "  label = tf.one_hot(label, 1000)\n",
        "  return img, label \n",
        "\n",
        "def preprocess_test(img): \n",
        "  img = tf.image.resize(img, (64, 64))\n",
        "  img = tf.cast(img, tf.float32) / 127.5 - 1\n",
        "  return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWT02QTA2F8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_train = dataset_train.map(preprocess_train, num_parallel_calls = -1).shuffle(1024).prefetch(-1).batch(batch_size).repeat()\n",
        "dataset_val = dataset_val.map(preprocess_train, num_parallel_calls = -1).shuffle(1024).prefetch(-1).batch(batch_size).repeat()\n",
        "dataset_test = dataset_test.map(preprocess_test, num_parallel_calls = -1).prefetch(-1).batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilBLs1cb2IM-",
        "colab_type": "code",
        "outputId": "555f7df7-f382-462e-ce4a-f582cee8291e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, LeakyReLU, BatchNormalization\n",
        "from keras.models import load_model \n",
        "from keras.regularizers import l2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWqst7dY2Kph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initializer = 'lecun_uniform'\n",
        "def make_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters=16, padding='same', kernel_size=(3,3), input_shape=(64,64,1)))\n",
        "    model.add(LeakyReLU(0.1))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "    # model.add(Dropout(0.5))\n",
        "    \n",
        "    model.add(Conv2D(filters=16, padding='same', kernel_size=(3,3), kernel_initializer=initializer))  \n",
        "    model.add(LeakyReLU(0.1))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "    model.add(Dropout(0.4))\n",
        "    \n",
        "    model.add(Conv2D(filters=32, padding='same', kernel_size=(3,3), kernel_initializer=initializer))  \n",
        "    model.add(LeakyReLU(0.1))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "    # model.add(Dropout(0.3))\n",
        "    \n",
        "    model.add(Conv2D(filters=32, padding='same', kernel_size=(3,3), kernel_initializer=initializer))  \n",
        "    model.add(LeakyReLU(0.1))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv2D(filters=64, padding='same', kernel_size=(3,3), kernel_initializer=initializer)) \n",
        "    model.add(BatchNormalization()) \n",
        "    model.add(LeakyReLU(0.1))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "    # model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Conv2D(filters=64, padding='same', kernel_size=(3,3), kernel_initializer=initializer))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.1))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Conv2D(filters=128, padding='same', kernel_size=(3,3), kernel_initializer=initializer))  \n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.1))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "    # model.add(Dropout(0.1))\n",
        "\n",
        "    model.add(Conv2D(filters=128, padding='same', kernel_size=(3,3), kernel_initializer=initializer))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.1))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Conv2D(filters=256, padding='same', kernel_size=(3,3), kernel_initializer=initializer))  \n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.1))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "    # model.add(Dropout(0.1))\n",
        "\n",
        "    model.add(Conv2D(filters=256, padding='same', kernel_size=(3,3), kernel_initializer=initializer))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.1))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1024))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(LeakyReLU(0.1))\n",
        "    model.add(Dense(1000))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlE2GUB52aHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = make_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPQ5zXQe2PGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Сохраним модель\n",
        "import cv2\n",
        "import keras\n",
        "from keras.models import save_model, load_model\n",
        "\n",
        "class ModelSaveCallback(keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self, file_name):\n",
        "        super(ModelSaveCallback, self).__init__()\n",
        "        self.file_name = file_name\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        epoch += 1\n",
        "        model.save(self.file_name.format(epoch))\n",
        "        print(model.save(self.file_name.format(epoch)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O50gO9Tj2T2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_from_file(model_filename, last_epoch):\n",
        "  model = load_model(model_filename)\n",
        "  epoch = last_epoch\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpEdjnJEJnEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ?\n",
        "optimizer=tf.keras.optimizers.Adam(learning_rate=0.001) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHcKWgp22xEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iRtaSMD2Vjo",
        "colab_type": "code",
        "outputId": "4b1e42a8-660a-40f1-fbea-636d3460b10d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        }
      },
      "source": [
        "model.fit(dataset_train, validation_data=dataset_val, epochs=20, steps_per_epoch=1000, validation_steps=200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1000 steps, validate on 200 steps\n",
            "Epoch 1/20\n",
            "1000/1000 [==============================] - 61s 61ms/step - loss: 0.0188 - acc: 0.9939 - val_loss: 0.0186 - val_acc: 0.9977\n",
            "Epoch 2/20\n",
            "1000/1000 [==============================] - 60s 60ms/step - loss: 0.0249 - acc: 0.9921 - val_loss: 0.0209 - val_acc: 0.9975\n",
            "Epoch 3/20\n",
            "1000/1000 [==============================] - 60s 60ms/step - loss: 0.0218 - acc: 0.9926 - val_loss: 0.0188 - val_acc: 0.9981\n",
            "Epoch 4/20\n",
            "1000/1000 [==============================] - 60s 60ms/step - loss: 0.0211 - acc: 0.9934 - val_loss: 0.0194 - val_acc: 0.9976\n",
            "Epoch 5/20\n",
            "1000/1000 [==============================] - 60s 60ms/step - loss: 0.0210 - acc: 0.9933 - val_loss: 0.0199 - val_acc: 0.9976\n",
            "Epoch 6/20\n",
            "1000/1000 [==============================] - 61s 61ms/step - loss: 0.0209 - acc: 0.9932 - val_loss: 0.0196 - val_acc: 0.9968\n",
            "Epoch 7/20\n",
            "1000/1000 [==============================] - 61s 61ms/step - loss: 0.0212 - acc: 0.9931 - val_loss: 0.0158 - val_acc: 0.9982\n",
            "Epoch 8/20\n",
            "1000/1000 [==============================] - 60s 60ms/step - loss: 0.0184 - acc: 0.9943 - val_loss: 0.0157 - val_acc: 0.9982\n",
            "Epoch 9/20\n",
            "1000/1000 [==============================] - 60s 60ms/step - loss: 0.0209 - acc: 0.9930 - val_loss: 0.0179 - val_acc: 0.9978\n",
            "Epoch 10/20\n",
            "1000/1000 [==============================] - 61s 61ms/step - loss: 0.0207 - acc: 0.9934 - val_loss: 0.0192 - val_acc: 0.9980\n",
            "Epoch 11/20\n",
            "1000/1000 [==============================] - 60s 60ms/step - loss: 0.0213 - acc: 0.9929 - val_loss: 0.0185 - val_acc: 0.9975\n",
            "Epoch 12/20\n",
            "1000/1000 [==============================] - 57s 57ms/step - loss: 0.0176 - acc: 0.9941 - val_loss: 0.0182 - val_acc: 0.9982\n",
            "Epoch 13/20\n",
            "1000/1000 [==============================] - 56s 56ms/step - loss: 0.0239 - acc: 0.9926 - val_loss: 0.0160 - val_acc: 0.9982\n",
            "Epoch 14/20\n",
            "1000/1000 [==============================] - 56s 56ms/step - loss: 0.0167 - acc: 0.9946 - val_loss: 0.0185 - val_acc: 0.9981\n",
            "Epoch 15/20\n",
            "1000/1000 [==============================] - 56s 56ms/step - loss: 0.0206 - acc: 0.9933 - val_loss: 0.0213 - val_acc: 0.9971\n",
            "Epoch 16/20\n",
            "1000/1000 [==============================] - 56s 56ms/step - loss: 0.0181 - acc: 0.9942 - val_loss: 0.0189 - val_acc: 0.9973\n",
            "Epoch 17/20\n",
            "1000/1000 [==============================] - 56s 56ms/step - loss: 0.0221 - acc: 0.9931 - val_loss: 0.0210 - val_acc: 0.9976\n",
            "Epoch 18/20\n",
            "1000/1000 [==============================] - 56s 56ms/step - loss: 0.0167 - acc: 0.9944 - val_loss: 0.0203 - val_acc: 0.9972\n",
            "Epoch 19/20\n",
            "1000/1000 [==============================] - 57s 57ms/step - loss: 0.0218 - acc: 0.9931 - val_loss: 0.0219 - val_acc: 0.9977\n",
            "Epoch 20/20\n",
            "1000/1000 [==============================] - 56s 56ms/step - loss: 0.0153 - acc: 0.9949 - val_loss: 0.0204 - val_acc: 0.9972\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff81ce44cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xQcAuqAvJAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = model.predict(dataset_test) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqqMj_XF2Y-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = submission.argmax(1)\n",
        "import pandas as pd  \n",
        "result = pd.read_csv(\"random_labels.csv\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8b7MPODvADB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result[\"Category\"] = submission \n",
        "result[\"Category\"] = result[\"Category\"].apply(lambda x: id_to_char[x])\n",
        "result.to_csv(\"submission.csv\", index = False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w7ooxhBvANX",
        "colab_type": "code",
        "outputId": "a4877a39-ab1c-46e7-cafc-e386e5206708",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "! kaggle competitions submit -c chinese-char-recognition-smmo19 -m 'Test' -f submission.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "100% 965k/965k [00:07<00:00, 140kB/s]\n",
            "Successfully submitted to Chinese character recognition"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}